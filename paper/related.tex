\section{Related Work}

We next discuss three lines of closely-related work on: (1)
the definitions of test dependence, (2)
testing techniques that assume test independence,
and (3) techniques to cope with test dependence.

\subsection{Test Dependence Definitions}

Treating test suites explicitly as
mathematical sets of tests~\cite{howden:ToC:1975}
and assuming test independence are common practice
in the testing literature~\cite{harroldetal:OOPSLA:2001, Orso:2004:SRT,
Briand:2009:ART, Zhang:2012:RMT, Nanda:2011:RTP,
Elbaum:2000:PTC:347324.348910, Kim:2002:HTP:581339.581357, Rummel:2005:TPR:1066677.1067016, Srivastava:2002:EPT:566172.566187, Jiang:2009:ART,
Steimann:2013, Zhang:2013:IMF, Jones:2002:VTI}.
%Treating test suites explicitly as mathematical sets of tests
%dates at least to Howden~\cite{howden:ToC:1975}.
%The execution order of tests in a suite is
%usually not considered: that is, test independence is assumed.
%
%
Nonetheless, some research has considered it.
Definitions in the testing literature are generally clear that
the conditions under which a test is executed may affect
its result. The importance of context in testing has been explored
in databases~\cite{Gray:1994:QGB:191843.191886,Chays:2000:FTD:347324.348954, kapfhammeretal:FSE:2003}, with results about test generation,
test adequacy criteria, etc., and mobile applications~\cite{Wang:2007:AGC}. For
the database domain, Kapfhammer and Soffa formally define
independent test suites and distinguish them from other
suites that ``can capture more of an application's interaction
with a database while requiring the constant monitoring of
database state and the potentially frequent re-computations
of test adequacy''~\cite{kapfhammeretal:FSE:2003}. 
%
%
Bergelson and Exman describe a form of test dependence
informally: given two tests that each pass, the
composite execution of these tests may still fail~\cite{bergelsonetal:EEE:2006}.
That is, if \textit{$t_1$} executed by itself passes and
\textit{$t_2$} executed by itself passes,
executing the sequence $\langle$\textit{$t_1$}, \textit{$t_2$}$\rangle$ in
the same context may fail.


In our previous work~\cite{testdependence}, we gave a formal definition
for test dependence based on test execution results.
Our definition differs from related work (e.g.,
Kapfhammer and Soffa's work~\cite{kapfhammeretal:FSE:2003}) by considering
test results rather than program and database states (which
may not affect the test results). 
%
%It would be possible to consider a test dependent if
%reordering could a?ect any internal computation or heap value
%(non-manifest dependence); but these internal details, such
%as order of elements in a hash table, might never a?ect any
%test result: they could be false dependences
However, the impact of test dependence on downstream
testing techniques is still unknown.
This paper answers this question by presenting
empirical evidence to assess the impact of
test dependence. We also describe new techniques
to enhance existing testing techniques to respect
test dependence and keep the test result consistent.
%In this paper, we use the our previous definition and focus
%on the manifest test dependence. \todo{more here}

\subsection{Techniques Assuming Test Independence}
%\todo{much of the related work is copied from the dt paper, needs revise.}
The assumption of test independence lies at the heart of most
techniques for automated regression test selection~\cite{harroldetal:OOPSLA:2001, Orso:2004:SRT,
Briand:2009:ART, Zhang:2012:RMT, Nanda:2011:RTP},
test case prioritization~\cite{Elbaum:2000:PTC:347324.348910, Kim:2002:HTP:581339.581357, Rummel:2005:TPR:1066677.1067016, Srivastava:2002:EPT:566172.566187, Jiang:2009:ART}, 
coverage-based fault localization~\cite{Steimann:2013, Zhang:2013:IMF, Jones:2002:VTI},
and test generation~\cite{PachecoLET2007, Wang:2007:AGC,
ZhangSBE2011} etc. 
We have thoroughly evaluated the impacts of test dependence
on test prioritization, test selection, and test parallelization
in Section~\ref{sec:impact}. Next, we discussed the impacts
on other testing techniques.


%Test prioritization seeks to reorder a test suite to detect
%software defects more quickly. 
%Early work in test
%prioritization~\cite{Wong:1997:SER:851010.856115,Rothermel:1999:TCP:519621.853398}
%laid the foundation for the most commonly used problem definition:
%consider the set of all permutations of a test suite and find the best
%award value for an objective function over that
%set~\cite{Elbaum:2000:PTC:347324.348910}.  The most common objective
%functions favor permutations where higher code coverage
%is achieved and more faults in the underlying
%program  are found with running fewer tests.
%Test independence is
%often explicitly asserted as
%a requirement for most test selection and prioritization work (e.g.,~\cite[p.~1500]{Rummel:2005:TPR:1066677.1067016}).
%Evaluations of selection and prioritization techniques~\cite[\emph{et alia}]{Rothermel:1999:TCP:519621.853398,Do:2010:ETC:1907658.1908088}
%are based in part on the test independence
%assumption as well as the assumption that the set of faults in the underlying
%program is known beforehand; the possibility that test dependence may
%interfere with these techniques is not studied.


Coverage-based fault localization techniques~\cite{Jones:2002:VTI}
often treat a test suite as a collection of test cases
whose result is \textit{independent} of the order of their
execution. They can also be impacted by test dependence.
In a recent evaluation of several coverage-based fault locators,
 Steimann et al.\ found fault locators' accuracy has been 
 affected by tests failed due to the violation of the test
 independence assumption~\cite{Steimann:2013}. 
 Compared to our work, Steimann et al.'s
 work focuses on identifying possible threats to validity
 in evaluating coverage-based fault locators, and does
 not present any results of the impact on other downstream
 testing techniques.


Test dependence also applies to automated test generation.
Most automated test generation
techniques~\cite{PachecoLET2007, Wang:2007:AGC,
ZhangSBE2011} do not take test dependence
into consideration. As shown in our
previous work~\cite{testdependence, RobinsonEPAL2011},
a large number of tests generated by Randoop are dependent.
We speculate that these dependences arise because automated
test generators generally create new tests
based on the program state after executing the previous test,
for the sake of test diversity and efficiency. 
When Randoop generates a nondeterministic test, it can disable the test but
leave it in the suite where it is executed in order to prevent other tests
that are dependent on it from beginning to fail~\cite{RobinsonEPAL2011}.
Exploring how to incorporate test dependence into the design of an automated
test generator is future work.


\subsection{Techniques to Cope with Test Dependence}

Most existing techniques are developed to prevent
test dependence, and few techniques can be used to alleviate
the its impact.
For example, testing frameworks provide mechanisms
for developers to define the context for tests.
JUnit, for example, provides means to
automatically execute setup and clean-up tasks
(\CodeIn{setUp()} and \CodeIn{tearDown()} in JUnit
3.x, and annotations \CodeIn{@Before} and \CodeIn{@After} in
JUnit 4.x). The latest release 4.11 of JUnit supports
executing tests in lexicographic order by test method name~\cite{junitordering}.
DepUnit~\cite{depunit} allows developers to define soft and hard dependences. Soft dependences control
test ordering, while hard dependences in addition control whether specific tests are
run at all.  TestNG~\cite{testng} 
allows dependence annotations and supports a variety of execution policies
that respect these dependences
such as sequential execution
in a single thread, execution of a single test class per thread, etc.\

However, ensuring that these advanced mechanisms are used properly is
beyond the scope and capability of any framework and tool. As
demonstrated in our previous work~\cite{testdependence},
programmers often do not use them properly and introduce
dependent tests. By contrast, this paper empirically
evaluated the impact of test dependence and proposed
techniques to cope their impact. Specifically,
our techniques designed in Section~\ref{sec:cope} automatically
enhances existing testing techniques to make them
produce a test ordering that preserves the test dependencies
and produce consistent results.
%
%What distinguishes our work from these approaches is that, while they allow dependences
%to be made explicit and respected during execution, they do not help developers
%\emph{cope with} the impact of dependences. 
On the other hand, our techniques (Section~\ref{sec:cope}) could co-exist
with such frameworks by generating annotations or detecting potential
test dependence.

%Haidry and Miller~\cite{10.1109/TSE.2012.26} proposed a set of
%test prioritization techniques that consider
%program dependence.  
%Their work explores dependences between program elements (rather
%dependences between tests)
%to improve existing test prioritization techniques.
%to make them produce a test ordering that preserves the test dependencies.
%Their work
%assumes that dependencies between tests are known (and are represented as
%partial orderings, such as that one test should be executed before another)
%without providing any empirical evidence of whether dependent tests
%exist in practice.
%\todo{Can/should we say that they did not motivate that their techniques
%  are needed in practice, but we have provided evidence of their value?}
%By contrast, our work takes the test dependence as input
%and enhances existing testing techniques to make them
%produce a test ordering that preserves the test dependencies.

%formally defines test dependence,
%studies the characteristics of real-world test dependence,
%shows how to detect dependent tests,
%and empirically evaluates whether dependent tests exist in real-world
%programs and
%their impact on existing test prioritization techniques.


%Muslu et al.~\cite{DBLP:conf/sigsoft/MusluSW11} proposed
%an algorithm to find bugs by executing each unit
%test in isolation. With a different focus,
%this work investigates the validity of the test independence assumption
%rather than finding new bugs,
%and presents five new results.
%Further, as indicated by our study and experiments, most dependent
%tests reveal weakness in the test code rather than bugs in the program. Thus,
%using test dependence may not achieve a high return in bug finding.
