\section{Introduction}


The test independence assumption states
that, in a test suite, all the test cases should be independent:
no test should affect any other test's result, and running
the tests in any order should produce the same test
results~\cite{testdependence}.
This assumption is important so that
tests behave consistently as designed. However,
as shown in our previous work~\cite{testdependence}, the test
independence assumption often does not hold in practice.
In other words, a test's result may depend on whether it runs after
some other tests.

%As a simple example, consider 
%a test suite containing two tests \CodeIn{A} and \CodeIn{B},
%where running \CodeIn{A} and then \CodeIn{B}
%leads to \CodeIn{A} passing, while running \CodeIn{B} and
%then \CodeIn{A} leads to \CodeIn{A} failing. We call
%\CodeIn{A} a dependent test (in the context of this test suite),
%since its result depends on whether it runs after \CodeIn{B} or not.

Test dependence causes inconsistent test results in different
execution orders and may lead to
some non-trivial impacts~\cite{testdependence}. First,
it leads to \textit{missed alarms} by masking faults in a program.
Specifically, executing a test suite in the default order does not
expose the fault, whereas executing the same test suite in
a different order does~\cite{clibug}. Second, 
it leads to \textit{false alarms}.
When a test should pass but fails after reordering due to
the dependence, people who are not aware of the dependence
can get confused and might report bugs~\cite{eclipsebug}. 



Our previous work provides concrete evidence to
show that test dependence exist and their impacts as causing
false alarms and missed alarms~\cite{testdependence}.
However, it is still unclear about whether test dependence can affect the
results of downstream testing techniques, and if it
impacts, how is the impact, and how should we cope with the impact.

This paper answers the above questions by empirically
investigating the impact of test dependence on
\prionum + \selnum + \parnum popular downstream testing techniques
and describing algorithms to cope with the impacts.

\subsection{Evaluating the Impact}

Test dependence can affect downstream testing
techniques that change a test suite and thereby change a test's
execution environment. 
Examples of such techniques include
test selection techniques (that identify a subset of the input
test suite to run during regression testing)~\cite{harroldetal:OOPSLA:2001, Orso:2004:SRT,
Briand:2009:ART, Zhang:2012:RMT, Nanda:2011:RTP, hsu09may},
test prioritization techniques (that reorder the input to
discover defects sooner)~\cite{Elbaum:2000:PTC:347324.348910,
Kim:2002:HTP:581339.581357, Rummel:2005:TPR:1066677.1067016, Srivastava:2002:EPT:566172.566187, Jiang:2009:ART},
and test parallelization
techniques (that schedule the input tests for execution across
multiple CPUs)~\cite{Misailovic:2007, Kim:2013:OUT}.
%
Most of these downstream testing techniques implicitly
assume that there are no test dependences in the input test
suite. Violation of this assumption, as we show happens in
practice, can cause unexpected output. As an example, test
prioritization may produce a reordered sequence of tests that
do not return the same results as they do when executed in
the default order.

To evaluate whether and how test dependence can impact
the downstream testing techniques, we implemented \prionum
test prioritization, \selnum test selection, and \parnum test
parallelization techniques, and evalauted each technique
on \subjnum real-world subject programs containing known dependent tests.
We measured the number of tests that yield different
results before and after applying a testing technique.
Our empirical results indicate that 
every evaluated downstream technique is affected by 
test dependence in a nonignorable manner. For example,
\todo{more details}

Our findings suggest that test dependence should no longer
be ignored in designing a downstream testing technique that
may change the original test execution order.



\subsection{Coping with the Impact}


%Test dependence results from interactions with other tests,
%as reflected in the execution environment. Tests may make
%implicit assumptions about their execution environment --
%values of global variables, contents of files, etc. A dependent
%test manifests when another alters the execution environment
%in a way that invalidates those assumptions.
%When writing tests, developers sometimes
%make mistakes as when they are writing
%other code. Even though frameworks such as JUnit provide
%ways to set up the environment for a test execution and clean
%up the environment afterward, they cannot ensure that it is
%done properly. This means that tests, like other code, will
%have unintended and unexpected behaviors in some cases.

Test dependence can be detected by systematically
executing a test suite in different orders, as algorithms implemented
in our tool DTDetector~\cite{testdependence} do. To cope
with the impact, when test dependence arises, existing downstream
testing techniques should respect such dependence and
keep the test results consistent. 

In this paper, we propose a family of
algorithms to make existing testing techniques dependence-aware.
\todo{describe the algorithms here}
The enhanced techniques take a test suite and the known
test dependence (possibly detected by a tool like DTDetector~\cite{testdependence}) as input. When producing a different test
execution order, our techniques enforce the test dependence
on the fly. For each dependent test, it ensures
that all other tests it depends on will be executed
before the dependent. Take test prioritization
as an example, given
a test suite in which test \CodeIn{A} depends on test \CodeIn{B},
when generating the prioritized order, our techniques check
whether test \CodeIn{B} has been executed when test \CodeIn{A}
is selected to be executed next. If not, our techniques
will first execute test \CodeIn{A} and then \CodeIn{B}.

%The key idea of our techniques is maintaining and c
%To enable downstream testing techniques to produce
%consistent results in the presence of test dependence, we
%we present a family of techniques to augument each downstream
%testing technique to respect test dependence. The key of
%our technique is to make each technique be aware of test dependence:
%when a dependent test has been selected to execute, it ensures
%all other tests it depends on will be executed before the
%dependent test.

%\end{enumerate}

For each evaluated downstream testing technique, we enhanced it
to form a dependence-aware technique. Each dependence-aware
testing technique is evaluated on real-world programs with dependent
tests. 
%We sent the generated dependence root cause explanation
%to program developers, and got positive feedback.
All enhanced techniques produce consistent and correct results without
compromising their effectiveness. Some techniques even produce
better results than its original forms.
\todo{more results here}

\subsection{Contributions}

This paper makes the following contributions.

\begin{itemize}

\item \textbf{Impact Assessment of Test Dependence.}
We present an empirical study to assess the impact of test dependence
on \prionum test prioritization, \selnum test selection, and \parnum
test parallelization techniques. We evaluated these 
testing techniques on \subjnum subject programs with a test suite containing
dependent tests. The results indicate that the results of
all techniques are affected by test dependence. This also
suggests that test dependence should no longer be overlooked
in designing new testing techniques (Section~\ref{sec:impact}).

\item \textbf{Techniques to Cope with Test Dependence.}
We propose a family of techniques to cope with test dependence.
Our techniques enhance existing test prioritization, selection,
and parallelization techniques to make them respect the test dependence
(Section~\ref{sec:cope}). 
%First, we describe a dynamic analysis to localize the dependence
%root cause and generate a concise report to help developers
%understand why the test dependence arises (Section~\ref{sec:coperoot}).

\item \textbf{Empirical Evaluation.} We evalauted the proposed
techniques on \subjnum subject programs containing test dependence, and
showed that the enhanced testing techniques are aware of test dependence;
they consistently produce the same results without compromising
the effectiveness (Section~\ref{sec:evaluation}).

\end{itemize}

