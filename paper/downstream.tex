Test prioritization, selection, and parallelization are three
important types of downstream testing techniques. They aim to
change an existing test suite in certain ways to make it run
faster, achieve coverage faster, detect faults earlier,
or fulfill other testing goals.

\subsubsection{Test prioritization.}

Test prioritization schedules test cases
for execution in an order that attempts to
increase their effectiveness.
One application of prioritization techniques involves
regression testing --- the retesting of software following modifications;
in this context, prioritization techniques can take advantage of information gathered about the previous
execution of test cases to obtain test case orderings.

Test prioritization techniques developed in the literature
fall into three major categories: (1) techniques
that order test cases based on their total coverage of
code components; (2) techniques that order test
cases based on their coverage of code components
not previously covered; (3) techniques that order test
cases based on their estimated ability to reveal faults
in the code components that they cover. In addition to the
test execution information, the third
category requires a comprehensive history of known
faults, which are often absent in practice and
approximated by seeded faults~\cite{}.


\begin{figure}
\centering
\setlength{\tabcolsep}{0.25\tabcolsep}
\begin{tabular}{|l|l|}
%\toprule
\hline
\textbf{Label} & \textbf{Technique Description} \\
\hline
T1 & Randomized ordering \\
T3 & Prioritize on coverage of statements \\
T4 & Prioritize on coverage of statements not yet covered\\
T5 & Prioritize on coverage of methods\\
T7 & Prioritize on coverage of functions not yet covered \\
%\bottomrule
\hline
%\textbf{Total}& &  & &  \\ 
%\hline
\end{tabular}
\caption{Five test prioritization techniques used
to assess the impact of dependent tests. These five
techniques are introduced in Table 1
of~\cite{Elbaum:2000:PTC:347324.348910}. (We use
the same labels as in~\cite{Elbaum:2000:PTC:347324.348910}. We did not
implement the other 9 test prioritization techniques
introduced in~\cite{Elbaum:2000:PTC:347324.348910}, since
they require a fault history that is not
available for our subject programs.)
}
\label{tab:testprio}
\end{figure}




In this paper, we focus on evaluating 5 well-known test prioritization
techniques from the first and second categories (summarized
in Figure~\ref{tab:testprio}).

%We wish to empirically investigate the validity of this unverified
%conventional wisdom, in order to understand whether
%test dependence can affect the results of test prioritization
%techniques.


\subsubsection{Test selection.}

Test selection techniques are often designed for improving
regression testing, which is the process of validating modified
software to detect whether new defects
have been introduced into previously tested code and
to provide confidence that modifications
are correct. 
%Since regression testing is an expensive process,
%researchers have proposed regression test selection (for short,
%test selection) techniques as a way to reduce some of this expense.
In general, test selection techniques attempt to select and execute
only a subset of the original test cases in a program to
reduce the testing cost.


%Since test selection techniques execute only a subset of the original
%test suite, they 
Test selection techniques may change the execution
environment of each selected test. 
Further, test selection is often used together with
test prioritization by prioritizing the selected
subset of tests~\cite{}. Thus, the test independence
assumption becomes particularly important to keep the
selected tests behaving the same as in the original test suite.

In this paper, we focus on evaluating 2 popular test
selection techniques (listed
in Figure~\ref{tab:testsel}) and conduct empirical evaluations
to investigate whether test dependence will affect their results.
For every test selection technique,
we also evalaute its effectiveness when combined with each test
prioritization technique listed in Figure~\ref{tab:testprio}.

\begin{figure}
\centering
\setlength{\tabcolsep}{0.25\tabcolsep}
\begin{tabular}{|l|l|}
%\toprule
\hline
\textbf{Label} & \textbf{Technique Description} \\
\hline
S1 & Select tests covering new/deleted/modified statements\\
S2 & Select tests covering new/deleted/modified methods\\
%\bottomrule
\hline
%\textbf{Total}& &  & &  \\ 
%\hline
\end{tabular}
\caption{Two test selection techniques used
to assess the impact of dependent tests. These
techniques are introduced in~\cite{}. These
two techniques form the basis of many other
popular test selection techniques~\cite{}.
}
\label{tab:testsel}
\end{figure}

\subsubsection{Test parallelization.}

Test execution parallelization (for short, test parallelization)
schedules the input tests for execution across
multiple CPUs to reduce the test time.
Such techniques are well adopted in
industry. For example, Visual Studio 2010 (and later)
supports a model of executing tests in parallel on a multi-CPU/core machine~\cite{}.
\todo{more evidence here}

Similarly, the test independence assumption is also critical to
test execution parallelization, since scheduling tests
to multiple machines may change the execution environment
of each test and then affect the result. 
%However, this critical
%assumption still remains unverified: to the best of our
%knowledge, all existing test parallelization techniques and
%tools~\cite{} implicitly assume that each test in a test
%suite is independent from one another.

In this paper, we focus on \parnum test parallelization techniques
(listed in Figure~\ref{tab:testpar}) and empirically investigate
the impacts of test dependence on them.

\begin{figure}
\centering
\setlength{\tabcolsep}{0.25\tabcolsep}
\begin{tabular}{|l|l|}
%\toprule
\hline
\textbf{Label} & \textbf{Technique Description} \\
\hline
P1 & Parallelize on test id\\
P2 & Random parallelization\\
P3 & Parallelize on test execution time\\
%\bottomrule
\hline
%\textbf{Total}& &  & &  \\ 
%\hline
\end{tabular}
\caption{Three test parallelization techniques used
to assess the impact of dependent tests. These
techniques are supported in industrial-strength
tools~\cite{}. \todo{explain each technique here.}
Each parallelization technique parameterized
by the number of available machines: $k$. We evaluate
each technique with $k$ = 2, 4, 8, and 16.
}
\label{tab:testpar}
\end{figure}

\begin{figure}
\centering
\setlength{\tabcolsep}{0.25\tabcolsep}
\begin{tabular}{|l|l|c|c|l|}
%\toprule
\hline
\textbf{Program} & \textbf{LOC} & \textbf{\#Tests} & \textbf{\#Auto Tests} & \textbf{Revision}
\\
\hline
%JFreechart & 92253 & \jfreecharttests & \jfreechartautotests& 1.0.15\\
%if we add JFreechart, need to change the total num, and num of subject program
%\midrule
\jt & 27183 & \jodatimetests
% 3875 is retrieved by running mvn test on the related revision
& -- &  b609d7d66d\\
XML Security & 18302 & \xmlsecuritytests & \xmlsecurityautotests& version 1.0.4 \\ 
Crystal & 4676 & \crystaltests & \crystalautotests& trunk version\\
Synoptic & 28872 & \synoptictests & \synopticautotests&  trunk version\\ 
JFreechart& xxx & xxx & xxx &  xxx \\ 
%\bottomrule
\hline
%\textbf{Total}& &  & &  \\ 
%\hline
\end{tabular}
\caption{Subject programs used in our evaluation.
Column ``\#Tests'' shows the number of human-written
unit tests. Column
``\#Auto Tests'' shows the number of 
unit tests generated by Randoop~\cite{PachecoLET2007}.
}
\label{tab:subjects}
\end{figure}
